{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATED SPLITS WITH SPECIES: Preprocess uniprot data (SPs that are only experimentally verified and verified by sequence analysis) and split into train/val/test with tokens for the species.\n",
    "\n",
    "First, go through and split the sequences into the signal peptide and the remainder of the sequence. \n",
    "Discard sequences where the signal peptide does not start at the first position. Then, discard sequences\n",
    "where the signal peptide is not between 10 and 70 amino acids, inclusive. Also discard sequences where \n",
    "the remaining sequence is not strictly longer than the signal peptide. \n",
    "\n",
    "In one training dataset, keep the first 100 amino acids of the mature protein. In another training dataset, only keep the first 95, 100, and 105 amino acids of the mature protein in the training dataset to vary the length of the protein sequences. This way, we get \"more\" training data if for each one.\n",
    "\n",
    "Remove examples where the SP is the same and the protein sequences are > 0.5 the same.\n",
    "\n",
    "For each example, also save the organism. All organisms with fewer than 5 examples get lumped together as token 0: 'AAUnknown' There are a total of 754 species tokens. \n",
    "\n",
    "There are a total of 32263 examples. \n",
    "\n",
    "Finally, shuffle the signal peptide/mature protein pairs and set aside 20% each as test and validation sets. The split is 19359/6452/6452. \n",
    "\n",
    "Minimal SP Length: 10 AA\n",
    "Maximal SP Length: 70 AA\n",
    " \n",
    "Defaults from SignalP http://www.cbs.dtu.dk/services/SignalP/instructions.php#limits\n",
    " \n",
    "Minimal Protein Length: Longer than Signal Peptide\n",
    "Maximal Protein Length: truncated to 70 -> according the SignalPâ€™s SI (below)\n",
    "https://images.nature.com/full/nature-assets/nmeth/journal/v8/n10/extref/nmeth.1701-S1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in datasets from csv\n",
    "dataset_75 = []\n",
    "\n",
    "filename = \"dataset_2.csv\"\n",
    "with open(filename, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    index = 0 # for appending to lists of similarities within the list \"sim\"\n",
    "    for j, line in enumerate(reader): # reads in each row of the csv\n",
    "        dataset_75.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in prot sequences from dataset\n",
    "df = pd.read_excel('../dataset.xls')\n",
    "si_c = df['Signal peptides'].values\n",
    "pr_c = df['Prot Sequences'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include triplets of all lengths\n",
    "triplets = []\n",
    "\n",
    "for si, pr in zip(si_c, pr_c):\n",
    "    if pr in dataset_75:\n",
    "        triplets.append((si, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22115"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove exact duplicates\n",
    "triplets = list(set(triplets))\n",
    "len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(a=1)\n",
    "random.shuffle(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13269, 4423, 4423)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = len(triplets) // 5\n",
    "test = triplets[-L:]\n",
    "val = triplets[-2 * L:-L]\n",
    "train = triplets[:-2*L]\n",
    "len(train), len(val), len(test)#check tokens\n",
    "#check similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure prot seq length of val and test are 100 aa\n",
    "# dataset where training has prot seqs of length 100\n",
    "\n",
    "train_len = [(si, pr[:100]) for si, pr in train]\n",
    "val_len = [(si, pr[:100]) for si, pr in val]\n",
    "test_len = [(si, pr[:100]) for si, pr in test]\n",
    "\n",
    "val = val_len\n",
    "test = test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of pairs with prot seq of length less than 100\n",
    "length = 0\n",
    "\n",
    "for t in train:\n",
    "    sp, pr = t\n",
    "    if len(pr) < 100:\n",
    "        length += 1\n",
    "\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../6-11_data/train_75.pkl', 'wb') as f:\n",
    "    pickle.dump(train_len, f)\n",
    "with open('../../6-11_data/validate_75.pkl', 'wb') as f:\n",
    "    pickle.dump(val, f)\n",
    "with open('../../6-11_data/test_75.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python code to remove duplicate elements\n",
    "def Remove(duplicate):\n",
    "    final_list = []\n",
    "    dup = []\n",
    "    for num in duplicate:\n",
    "        if num not in final_list:\n",
    "            final_list.append(num)\n",
    "        else:\n",
    "            dup.append(num)\n",
    "    return dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13269\n",
      "13269\n",
      "13269\n",
      "39807\n",
      "39746\n"
     ]
    }
   ],
   "source": [
    "# vary prot length in training set\n",
    "maximum = 105\n",
    "leng1 = 0\n",
    "leng2 = 0\n",
    "train_vlen = []\n",
    "train_vlen1 = []\n",
    "train_vlen2 = []\n",
    "\n",
    "####### keep train with og lengths, len, len-5, len-10 (smaller of 105 or actual protein length for len)\n",
    "for t in train:\n",
    "    si, pr = t\n",
    "    if len(pr) < maximum:\n",
    "        leng1 = len(pr) - 10\n",
    "        leng2 = len(pr) - 5\n",
    "        train_vlen.append((si, pr[:leng1]))\n",
    "        train_vlen1.append((si, pr[:leng2]))\n",
    "        train_vlen2.append((si, pr))\n",
    "    else:\n",
    "        train_vlen.append((si, pr[:95]))\n",
    "        train_vlen1.append((si, pr[:100]))\n",
    "        train_vlen2.append((si, pr[:105]))\n",
    "\n",
    "print(len(train_vlen))\n",
    "print(len(train_vlen1))\n",
    "print(len(train_vlen2))\n",
    "\n",
    "train = []\n",
    "train = train_vlen + train_vlen1 + train_vlen2\n",
    "\n",
    "print(len(train))\n",
    "# Remove exact duplicates\n",
    "train = list(set(train))\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump data with sp, prot, and species in *_species_augmented.pkl files (training dataset with varied prot seq lengths of \n",
    "# 95, 100, and 105)\n",
    "\n",
    "with open('../../6-11_data/train_augmented_75.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MKKLISNDVTPEEIFYQRRKIIKAFGLSAVATALPTFSFA',\n",
       " 'QESSDLKALEYKKSTESTLILTPENKVTGYNNFYEFGVDKGSPAHYAKNFQVNPWKLDIGGEVENPFTLNYDQLFTQFPLEERIYRFRCVEAWAMVVPWI')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../6-11_data/train_75.pkl', 'rb') as f:\n",
    "    t = pickle.load(f)\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove species from datasets to be dumped into train.pkl, test.pkl, and validate.pkl\n",
    "# has varied lengths of 95, 100, and 105\n",
    "\n",
    "train_nosp = [(si, pr) for si, pr in train]\n",
    "\n",
    "with open('../../6-11_data/train_augmented_75.pkl', 'wb') as f:\n",
    "    pickle.dump(train_nosp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train = [('MRLSTAQLIAIAYYMLSIGATVPQVDG', 'QGETEEALIQKRSYDYYQEPCDDYPQQQQQQEPCDYPQQQQQEEPCDYPQQQPQEPCDYPQQPQEPCDYPQQPQEPCDYPQQPQEPCDNPPQPDV', 121), ('MLTPRVLRALGWTGLFFLLLSPSNVLG', 'ASLSRDLETPPFLSFDPSNISINGAPLTEVPHAPSTESVSTNSESTNEHTITETTGKNAYIHNNASTDKQNANDTHKTPNILCDTEEVFVFLNET', 260)]\n",
    "leng1 = 0\n",
    "leng2 = 0\n",
    "lst = []\n",
    "\n",
    "for t in train:\n",
    "    si, pr, sp = t\n",
    "    leng1 = len(pr) - 10\n",
    "    leng2 = len(pr) - 5\n",
    "    lst.append(pr[:leng1])\n",
    "    lst.append(pr[:leng2])\n",
    "    lst.append(pr)\n",
    "\n",
    "print(len(lst))\n",
    "list(set(lst))\n",
    "print(len(lst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
